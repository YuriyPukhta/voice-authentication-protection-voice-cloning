{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model to recognise voice of people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torchvision.transforms.functional import normalize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.columns.combined_triplet_dataset_column import CombinedTripletDatasetColumn\n",
    "import librosa\n",
    "\n",
    "from src.columns.combined_dataset_column import CombinedDatasetColumn\n",
    "from src.transform.transform import CustomAdjustDurationTransform, ResampleTransform, ToMelSpectrogramTransform\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.transform.composite_transformation import CompositeTransformation\n",
    "\n",
    "\n",
    "class CombinedTripletSoundDS(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        voiceDataset,\n",
    "        original_voice_data_path,\n",
    "        generated_voice_data_path,\n",
    "        sample_rate=16000,\n",
    "        duration=3,\n",
    "        transform=None\n",
    "    ):\n",
    "        self.voiceDataset = voiceDataset\n",
    "        self.original_voice_data_path = str(original_voice_data_path)\n",
    "        self.generated_voice_data_path = str(generated_voice_data_path)\n",
    "        self.duration = duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = None\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = CompositeTransformation(\n",
    "                [\n",
    "                    ResampleTransform(target_sample_rate=sample_rate),\n",
    "                    CustomAdjustDurationTransform(duration_seconds=duration),\n",
    "                    ToMelSpectrogramTransform(\n",
    "                        sample_rate=sample_rate, n_mels=64, n_fft=512)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voiceDataset)\n",
    "\n",
    "    def _get_sgram(self, audio_file):\n",
    "        audio, sample_rate = librosa.load(audio_file, sr=self.sample_rate)\n",
    "        spectrogram = self.transform.transform((audio, sample_rate))\n",
    "        spectrogram = spectrogram.unsqueeze(0)\n",
    "        return spectrogram\n",
    "\n",
    "    def _get_sample_path(self, path, source):\n",
    "        data_path = self.original_voice_data_path if source == 'original' else self.generated_voice_data_path\n",
    "        return data_path + path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, pos_path, neg_path, anchor_source, pos_source, neg_source = self.voiceDataset.iloc[idx][\n",
    "            [\n",
    "\n",
    "                CombinedTripletDatasetColumn.ANCHOR_PATH,\n",
    "                CombinedTripletDatasetColumn.POS_PATH,\n",
    "                CombinedTripletDatasetColumn.NEG_PATH,\n",
    "                CombinedTripletDatasetColumn.SOURCE_ANCHOR,\n",
    "                CombinedTripletDatasetColumn.SOURCE_POS,\n",
    "                CombinedTripletDatasetColumn.SOURCE_NEG,\n",
    "            ]\n",
    "        ].values.tolist()\n",
    "\n",
    "        anchor_file = self._get_sample_path(anchor_path, anchor_source)\n",
    "        pos_file = self._get_sample_path(pos_path, pos_source)\n",
    "        neg_file = self._get_sample_path(neg_path, neg_source)\n",
    "        \n",
    "        anchor_sgram = self._get_sgram(anchor_file)\n",
    "        pos_sgram = self._get_sgram(pos_file)\n",
    "        neg_sgram = self._get_sgram(neg_file)\n",
    "        return anchor_sgram, pos_sgram, neg_sgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = './dataset/'\n",
    "DATA_PATH = './data/'\n",
    "MODEL_SAVE_PATH = './model_save/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = './dataset/'\n",
    "DATA_PATH = './data/'\n",
    "MODEL_SAVE_PATH = './model_save/'\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transform.transform import CustomAdjustDurationTransform, ResampleTransform, CustomAdjustDurationTransform, ShiftWrapperTransform, ToMelSpectrogramTransform, AirAbsorptionWrapperTransform, EqualizerWrapperTransform\n",
    "from src.transform.composite_transformation import CompositeTransformation\n",
    "\n",
    "train_transform = CompositeTransformation(\n",
    "\t[\n",
    "\t\tResampleTransform(target_sample_rate=SAMPLE_RATE),\n",
    "\t\tCustomAdjustDurationTransform(duration_seconds=3),\n",
    "\t\tShiftWrapperTransform(min_shift=-0.2, max_shift=0.2),\n",
    "\t\tAirAbsorptionWrapperTransform(),\n",
    "\t\tEqualizerWrapperTransform(),\n",
    "\t\tToMelSpectrogramTransform(sample_rate=SAMPLE_RATE, n_mels=64, n_fft=512)\n",
    "\t]\n",
    ")\n",
    "\n",
    "test_transform = CompositeTransformation(\n",
    "\t[\n",
    "\t\tResampleTransform(target_sample_rate=SAMPLE_RATE),\n",
    "\t\tCustomAdjustDurationTransform(duration_seconds=3),\n",
    "\t\tShiftWrapperTransform(min_shift=-0.2, max_shift=0.2),\n",
    "\t\tAirAbsorptionWrapperTransform(),\n",
    "\t\tEqualizerWrapperTransform(),\n",
    "\t\tToMelSpectrogramTransform(sample_rate=SAMPLE_RATE, n_mels=64, n_fft=512)\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_PATH + \"train_combined_triplet_dataset.csv\")\n",
    "train_ds = CombinedTripletSoundDS(train_df, original_voice_data_path=DATA_PATH + \"validated_16000/\", generated_voice_data_path=DATA_PATH + \"generated_16000/\", transform=test_transform)\n",
    "\n",
    "test_df = pd.read_csv(DATASET_PATH + \"test_combined_triplet_dataset.csv\")\n",
    "test_ds = CombinedTripletSoundDS(test_df, original_voice_data_path=DATA_PATH + \"validated_16000/\", generated_voice_data_path=DATA_PATH + \"generated_16000/\", transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "\t\tdef __init__(self):\n",
    "\t\t\t\tsuper(SiameseNetwork, self).__init__()\n",
    "\t\t\t\tself.conv_layers = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Conv2d(1, 32, kernel_size=3),\n",
    "\t\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\t\t\t\tnn.Conv2d(32, 64, kernel_size=3),\n",
    "\t\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\t\t\t\tnn.Conv2d(64, 128, kernel_size=3),\n",
    "\t\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\t\t\t\tnn.Conv2d(128, 256, kernel_size=3),\n",
    "\t\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.fc_layers = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Linear(4608, 1024),\n",
    "\t\t\t\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\tself.final = nn.Linear(1024, 2)\n",
    "\n",
    "\t\tdef forward_once(self, x):\n",
    "\n",
    "\t\t\t\tx = self.conv_layers(x)\n",
    "\t\t\t\tx = x.view(x.size(0), -1)\n",
    "\t\t\t\tx = self.fc_layers(x)\n",
    "\t\t\t\treturn x\n",
    "\n",
    "\t\tdef forward(self, anchor_input, pos_input, neg_input):\n",
    "\t\t\tanchor_output= self.forward_once(anchor_input)\n",
    "\t\t\tpos_output = self.forward_once(pos_input)\n",
    "\t\t\tneg_output = self.forward_once(neg_input)\n",
    "\t\t\treturn  anchor_output, pos_output, neg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_siamese_model_triplet_loss(model, dataloader, criterion, device='cpu'):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0.0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "\t\t\tanchor_sgram, pos_sgram, neg_sgram = batch\n",
    "\t\t\tanchor_sgram = anchor_sgram.to(device)\n",
    "\t\t\tpos_sgram = pos_sgram.to(device)\n",
    "\t\t\tneg_sgram = neg_sgram.to(device)\n",
    "\t\t\tanchor_output, pos_output, neg_output = model(anchor_sgram, pos_sgram, neg_sgram)\n",
    "\t\t\tloss = criterion(anchor_output, pos_output, neg_output)\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\n",
    "\tavg_loss = total_loss / len(dataloader)\n",
    "\n",
    "\treturn avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_model_triplet_loss(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "\tmodel.train()\n",
    "\ttotal_loss = 0.0\n",
    "\n",
    "\tfor batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "\t\tanchor_sgram, pos_sgram, neg_sgram = batch\n",
    "\t\tanchor_sgram = anchor_sgram.to(device)\n",
    "\t\tpos_sgram = pos_sgram.to(device)\n",
    "\t\tneg_sgram = neg_sgram.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tanchor_output, pos_output, neg_output = model(anchor_sgram, pos_sgram, neg_sgram)\n",
    "\t\tloss = criterion(anchor_output, pos_output, neg_output)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\tavg_loss = total_loss / len(dataloader)\n",
    "\t\n",
    "\treturn avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=margin, p=2, eps=1e-7)\n",
    "\n",
    "    def forward(self,anchor, positive, negative):\n",
    "        loss=self.triplet_loss(anchor, positive, negative)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "siamese_model = SiameseNetwork().to('cuda')\n",
    "criterion = TripletLoss().to('cuda')\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=0.0001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.5343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.4521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.5462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.4068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.3116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss, accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 3\u001b[0m \t_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_siamese_model_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiamese_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \t_test_loss \u001b[38;5;241m=\u001b[39m evaluate_siamese_model_triplet_loss(siamese_model, test_dl, criterion, device)\n",
      "Cell \u001b[1;32mIn[208], line 5\u001b[0m, in \u001b[0;36mtrain_siamese_model_triplet_loss\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m \tanchor_sgram, pos_sgram, neg_sgram \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      7\u001b[0m \tanchor_sgram \u001b[38;5;241m=\u001b[39m anchor_sgram\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 70\u001b[0m, in \u001b[0;36mCombinedTripletSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     67\u001b[0m pos_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_path(pos_path, pos_source)\n\u001b[0;32m     68\u001b[0m neg_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_path(neg_path, neg_source)\n\u001b[1;32m---> 70\u001b[0m anchor_sgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m pos_sgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sgram(pos_file)\n\u001b[0;32m     72\u001b[0m neg_sgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sgram(neg_file)\n",
      "Cell \u001b[1;32mIn[7], line 45\u001b[0m, in \u001b[0;36mCombinedTripletSoundDS._get_sgram\u001b[1;34m(self, audio_file)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_sgram\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_file):\n\u001b[0;32m     44\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_file, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m---> 45\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m spectrogram\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spectrogram\n",
      "File \u001b[1;32md:\\project\\voice-authentication-protection-voice-cloning\\src\\transform\\composite_transformation.py:11\u001b[0m, in \u001b[0;36mCompositeTransformation.transform\u001b[1;34m(self, audio_signal)\u001b[0m\n\u001b[0;32m      8\u001b[0m transformed_signal \u001b[38;5;241m=\u001b[39m (audio_signal[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(), audio_signal[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m---> 11\u001b[0m     transformed_signal \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtransformed_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_signal\n",
      "File \u001b[1;32md:\\project\\voice-authentication-protection-voice-cloning\\src\\transform\\transform.py:153\u001b[0m, in \u001b[0;36mAirAbsorptionWrapperTransform.__call__\u001b[1;34m(self, samples, sr)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples: NDArray[np\u001b[38;5;241m.\u001b[39mfloat32], sr: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 153\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mair_absorption_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (samples, sr)\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\audiomentations\\core\\transforms_interface.py:93\u001b[0m, in \u001b[0;36mBaseWaveformTransform.__call__\u001b[1;34m(self, samples, sample_rate)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_mono:\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MonoAudioNotSupportedException(\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m only supports multichannel audio, not mono audio\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     90\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     91\u001b[0m             )\n\u001b[0;32m     92\u001b[0m         )\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\audiomentations\\augmentations\\air_absorption.py:163\u001b[0m, in \u001b[0;36mAirAbsorption.apply\u001b[1;34m(self, samples, sample_rate)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Apply using STFT\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(samples\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 163\u001b[0m     stft \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Compute mask\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(linear_target_attenuations, (stft\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\librosa\\core\\spectrum.py:378\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    376\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 378\u001b[0m     \u001b[43mstft_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moff_start\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moff_start\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m fft\u001b[38;5;241m.\u001b[39mrfft(\n\u001b[0;32m    379\u001b[0m         fft_window \u001b[38;5;241m*\u001b[39m y_frames[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s:bl_t], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, accuracy, precision, recall, f1 = [], [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "\t_train_loss = train_siamese_model_triplet_loss(siamese_model, train_dl, criterion, optimizer, device)\n",
    "\tprint(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {_train_loss:.4f}\")\n",
    "\t_test_loss = evaluate_siamese_model_triplet_loss(siamese_model, test_dl, criterion, device)\n",
    "\tprint(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {_test_loss:.4f}\")\n",
    "\tloss.append([_train_loss, _test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, base_path = './model_save/'):\n",
    "\tmodel.cpu()\n",
    "\tmodel.eval()\n",
    "\ttorch.save(model, base_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(siamese_model, \"contrast_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_siamese_model(anchor_sgram, pos_sgram, neg_sgram, loss):\n",
    "\tplt.style.use(\"dark_background\")\n",
    "\tplt.figure(figsize=(10, 2), facecolor=\"#1e1e1e\")\n",
    "\tplt.subplot(1, 2, 1)\n",
    "\tplt.title(\"Anchor Sgram\")\n",
    "\tplt.imshow(anchor_sgram,  origin=\"lower\", aspect=\"auto\")\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\ttype = \"Positive\" if label.item() == 0 else \"Negative\"\n",
    "\tplt.title(f\"{type} Sgram\")\n",
    "\tplt.imshow(posneg_sgram,  origin=\"lower\", aspect=\"auto\")\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\tplt.show()\n",
    "\tprint(f\"Similarity: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(model, anchor_sgram, posneg_sgram):\n",
    "\tmodel.eval()\n",
    "\tmodel.to(device)\n",
    "\twith torch.no_grad():\n",
    "\n",
    "\n",
    "\t\tanchor_sgram = anchor_sgram.unsqueeze(0).to(device)\n",
    "\t\tpos_sgram = posneg_sgram.unsqueeze(0).to(device)\n",
    "\t\tneg_sgram = posneg_sgram.unsqueeze(0).to(device)\n",
    "\t\tanchor_output, pos_output, neg_output = model(anchor_sgram, pos_sgram,neg_sgram )\n",
    "\t\tprint(f\"pos: {F.pairwise_distance(anchor_output, pos_output).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 8.358107566833496\n",
      "pos: 15.434844970703125\n"
     ]
    }
   ],
   "source": [
    "anchor_sgram, pos_sgram, neg_sgram = train_ds[0]\n",
    "evaluate(siamese_model, anchor_sgram, pos_sgram)\n",
    "evaluate(siamese_model, anchor_sgram, neg_sgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 8.049684524536133\n",
      "pos: 11.992538452148438\n"
     ]
    }
   ],
   "source": [
    "anchor_sgram, pos_sgram, neg_sgram = train_ds[0]\n",
    "evaluate(siamese_model, anchor_sgram, pos_sgram)\n",
    "evaluate(siamese_model, anchor_sgram, neg_sgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 8.399796485900879\n",
      "pos: 14.188668251037598\n"
     ]
    }
   ],
   "source": [
    "anchor_sgram, pos_sgram, neg_sgram = train_ds[0]\n",
    "evaluate(siamese_model, anchor_sgram, pos_sgram)\n",
    "evaluate(siamese_model, anchor_sgram, neg_sgram)\n",
    "#plot_siamese_model(anchor_sgram.squeeze(0), posneg_sgram.squeeze(0), label, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 4.1458892822265625\n",
      "pos: 11.802881240844727\n"
     ]
    }
   ],
   "source": [
    "anchor_sgram, pos_sgram, neg_sgram = test_ds[367]\n",
    "evaluate(siamese_model, anchor_sgram, pos_sgram)\n",
    "evaluate(siamese_model, anchor_sgram, neg_sgram)\n",
    "#plot_siamese_model(anchor_sgram.squeeze(0), posneg_sgram.squeeze(0), label, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (final): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, m): \n",
    "      self.hook = m.register_forward_hook(self.hook_func)\n",
    "    def hook_func(self, module, input, output):\n",
    "      self = (output.cpu()).data\n",
    "    def remove(self):\n",
    "      self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = siamese_model._modules.get('conv_layers')[-2]\n",
    "act_maps = Hook(final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = siamese_model._modules.get('conv_layers')(anchor_sgram.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hook' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[413], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mact_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Hook' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "grad = act_maps.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad  = torch.tensor(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradients = torch.mean(grad, dim=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pooled_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_maps.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 2, 9])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hook' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[390], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mact_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Hook' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "act_maps.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(prediction.shape[1]):\n",
    "\tactivations[:, i, :, :] *= pooled_gradients[i]\n",
    "\t\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = torch.where(heatmap > 0, heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# Reshape & Convert Tensor to numpy\n",
    "heatmap = heatmap.squeeze()\n",
    "heatmap = heatmap.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Resize Heatmap\n",
    "heatmap = cv2.resize(heatmap, image_size)\n",
    "# Convert to [0,255]\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "\n",
    "if figname is not None:\n",
    "\tplt.figure(figsize=figsize);\n",
    "\tfig = plt.imshow(heatmap);\n",
    "\tfig.axes.get_xaxis().set_visible(False)\n",
    "\tfig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\tplt.savefig(figname, dpi=300, format='png', \n",
    "\t\t\tbbox_inches='tight', pad_inches=0.1,\n",
    "\t\t\tfacecolor='auto', edgecolor='auto',\n",
    "\t\t\tbackend=None, \n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "weight_softmax_params = list(siamese_model._modules.get('fc').parameters())\n",
    "weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())\n",
    "idx = topk(pred_prob,1)[1].int()\n",
    "overlay = CAM(act_maps.features, weight_softmax, idx )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (final): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'pos_input' and 'neg_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[358], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiamese_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m188\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m188\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\Yuriy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'pos_input' and 'neg_input'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(siamese_model, (1, 64, 188),  (1, 64, 188))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model._modules.get('layer4')\n",
    "act_maps = Hook(final_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
